{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 435,
      "metadata": {
        "id": "zTUtpgHAfLJi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The below cell contains two functions that take inputs `qx` or `kx` and return transformed `qx` or transformed `kx`. The below cell is the easiest way to understand about what goes on in `rotary positional embedding` without much going into mathematics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JqU3v5cvqq8D"
      },
      "outputs": [],
      "source": [
        "\"\"\"How we estimate the transformation\n",
        "\"\"\"\n",
        "\n",
        "def Compute_angles(head_dim:int, seq_len:int, device:str = 'cpu', theta:int = 10000)->torch.tensor:\n",
        "\n",
        "  theta_numerator = torch.arange(0, head_dim, 2).float(); '(hd // 2, )'\n",
        "  theta = 1.0 / (theta ** (theta_numerator / head_dim)).to(device); '(hd // 2, )'\n",
        "\n",
        "\n",
        "  m = torch.arange(seq_len).to(device); \"(T, )\"\n",
        "  angles = torch.outer(m, theta); '(T, hd // 2)'\n",
        "\n",
        "  return angles.float()\n",
        "\n",
        "def apply_rotation(x:torch.tensor, angles:torch.tensor):\n",
        "  B, T, nh, hd = x.shape\n",
        "  x_re = x.reshape(B, T, nh, hd // 2, 2);'(B, T, nh, hd) ---> (B, T, nh, hd // 2, 2)'\n",
        "\n",
        "  angles = angles.unsqueeze(0).unsqueeze(2); '(T, hd // 2) ---> (1, T, 1, hd // 2)'\n",
        "  angles = angles.repeat(B, 1, nh, 1); '(1, T, 1, hd // 2) ---> (B, T, nh, hd // 2)'\n",
        "\n",
        "\n",
        "\n",
        "  cos_angles = torch.cos(angles); '(B, T, nh, hd // 2)'\n",
        "\n",
        "  sin_angles = torch.sin(angles); '(B, T, nh, hd // 2)'\n",
        "\n",
        "  cos_angles = cos_angles.unsqueeze(4).unsqueeze(5); '(B, T, nh, hd // 2) ---> (B, T, nh, hd // 2, 1, 1)'\n",
        "  sin_angles = sin_angles.unsqueeze(4).unsqueeze(5); '(B, T, nh, hd // 2) ---> (B, T, nh, hd // 2, 1, 1)'\n",
        "\n",
        "  cos_sin1 = torch.cat((cos_angles, -1 * sin_angles), -1); '(B, T, nh, hd // 2, 1, 1) ---> (B, T, nh, hd // 2, 1, 2)'\n",
        "\n",
        "  cos_sin2 = torch.cat((sin_angles, cos_angles), -1); '(B, T, nh, hd // 2, 1, 1) ---> (B, T, nh, hd // 2, 1, 2)'\n",
        "\n",
        "  rotation_matrix = torch.cat((cos_sin1, cos_sin2), dim = -2); '(B, T, nh, hd // 2, 1, 2) ---> (B, T, nh, hd // 2, 2, 2)'\n",
        "\n",
        "  x_re = x_re.unsqueeze(5); '(B, T, nh, hd // 2, 2) ---> (B, T, nh, hd // 2, 2, 1)'\n",
        "\n",
        "  x_rotated = rotation_matrix @ x_re; '(B, T, nh, hd // 2, 2, 2) --> (B, T, nh, hd // 2, 2, 1)'\n",
        "\n",
        "  x_rotated = x_rotated.squeeze(); '(B, T, nh, hd // 2, 2, 1) ---> (B, T, nh, hd // 2, 2)'\n",
        "\n",
        "  x_rotated = x_rotated.reshape(B, T, nh, hd); '(B, T, nh, hd // 2, 2) ---> (B, T, nh, hd)'\n",
        "\n",
        "  assert x_rotated.shape == x.shape; 'Shapes must be same'\n",
        "\n",
        "  return x_rotated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gDn7O0X3KIl"
      },
      "source": [
        "#### The below cell contains two functions that take inputs `qx` or `kx` and return transformed `qx` or transformed `kx`. I took the code from `llamma2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avjaVpqqxqEN"
      },
      "outputs": [],
      "source": [
        "\"\"\"How they estimate the transformation\n",
        "\"\"\"\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
        "    \"\"\"\n",
        "    Precompute the frequency tensor for complex exponentials (cis) with given dimensions.\n",
        "\n",
        "    This function calculates a frequency tensor with complex exponentials using the given dimension 'dim'\n",
        "    and the end index 'end'. The 'theta' parameter scales the frequencies.\n",
        "    The returned tensor contains complex values in complex64 data type.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Dimension of the frequency tensor.\n",
        "        end (int): End index for precomputing frequencies.\n",
        "        theta (float, optional): Scaling factor for frequency computation. Defaults to 10000.0.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Precomputed frequency tensor with complex exponentials.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    t = torch.arange(end, device=freqs.device)  # type: ignore\n",
        "    freqs = torch.outer(t, freqs).float()  # type: ignore\n",
        "\n",
        "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
        "    return freqs_cis\n",
        "\n",
        "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
        "    \"\"\"\n",
        "    Reshape frequency tensor for broadcasting it with another tensor.\n",
        "\n",
        "    This function reshapes the frequency tensor to have the same shape as the target tensor 'x'\n",
        "    for the purpose of broadcasting the frequency tensor during element-wise operations.\n",
        "\n",
        "    Args:\n",
        "        freqs_cis (torch.Tensor): Frequency tensor to be reshaped.\n",
        "        x (torch.Tensor): Target tensor for broadcasting compatibility.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Reshaped frequency tensor.\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If the frequency tensor doesn't match the expected shape.\n",
        "        AssertionError: If the target tensor 'x' doesn't have the expected number of dimensions.\n",
        "    \"\"\"\n",
        "    print(freqs_cis.shape, x.shape)\n",
        "    ndim = x.ndim\n",
        "    assert 0 <= 1 < ndim\n",
        "    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n",
        "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
        "    return freqs_cis.view(*shape)\n",
        "\n",
        "\n",
        "def apply_rotary_emb(\n",
        "    xq: torch.Tensor,\n",
        "    freqs_cis: torch.Tensor,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Apply rotary embeddings to input tensors using the given frequency tensor.\n",
        "\n",
        "    This function applies rotary embeddings to the given query 'xq' and key 'xk' tensors using the provided\n",
        "    frequency tensor 'freqs_cis'. The input tensors are reshaped as complex numbers, and the frequency tensor\n",
        "    is reshaped for broadcasting compatibility. The resulting tensors contain rotary embeddings and are\n",
        "    returned as real tensors.\n",
        "\n",
        "    Args:\n",
        "        xq (torch.Tensor): Query tensor to apply rotary embeddings.\n",
        "        xk (torch.Tensor): Key tensor to apply rotary embeddings.\n",
        "        freqs_cis (torch.Tensor): Precomputed frequency tensor for complex exponentials.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings.\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
        "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
        "    return xq_out.type_as(xq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "B, T, C = 4, 16, 32 #B = Batch size, T = sequence length, C = embedding size(dim in Llamma2),\n",
        "\n",
        "torch.manual_seed(1)\n",
        "embedding = torch.randn((B, T, C))\n",
        "\n",
        "q_weight = torch.randn((C, C))\n",
        "\n",
        "k_weight = torch.randn((C, C))\n",
        "\n",
        "qx = embedding @ q_weight #(B, T, C) @ (C, C) ---> (B, T, C) @ (B, C, C)---> (B, T, C)\n",
        "kx = embedding @ k_weight #(B, T, C) @ (C, C) ---> (B, T, C) @ (B, C, C)---> (B, T, C)\n",
        "\n",
        "nh = 4 #nh = number of head\n",
        "\n",
        "qx = qx.reshape(B, T, nh, C // nh) #C // nh = hd\n",
        "kx = kx.reshape(B, T, nh, C // nh)\n",
        "qx.shape, kx.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iEXUf1V3SMv"
      },
      "source": [
        "#Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-93uCoFux-5y",
        "outputId": "01f3272b-3ade-4023-a0aa-d98a8672352e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 4]) torch.Size([4, 16, 4, 4])\n",
            "torch.Size([16, 4]) torch.Size([4, 16, 4, 8]) torch.Size([4, 16, 4, 8])\n",
            "torch.Size([16, 4]) torch.Size([4, 16, 4, 8]) torch.Size([4, 16, 4, 8])\n",
            "True\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(tensor([ 4.6672, -0.7728, -1.5132,  2.0685,  2.6615,  1.2631, -5.4974, -0.7179]),\n",
              " tensor([ 4.6672, -0.7728, -1.5132,  2.0685,  2.6615,  1.2631, -5.4974, -0.7179]))"
            ]
          },
          "execution_count": 464,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "hs = C // nh #Head size or head dimension(hd)\n",
        "\n",
        "angles_their = precompute_freqs_cis(hs, T)\n",
        "rotated_qx_their = apply_rotary_emb(qx, angles_their)\n",
        "print(angles_their.shape, qx.shape, rotated_qx_their.shape)\n",
        "\n",
        "angles_ours = Compute_angles(hs, T)\n",
        "rotated_qx_ours = apply_rotation(qx, angles_ours)\n",
        "print(angles_ours.shape, qx.shape, rotated_qx_ours.shape)\n",
        "\n",
        "print(torch.allclose(rotated_qx_their, rotated_qx_ours))\n",
        "\n",
        "rotated_qx_their[3, 3, 1, :], rotated_qx_ours[3, 3, 1, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZYLxPICyWQE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
