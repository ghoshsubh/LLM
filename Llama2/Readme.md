
# Open Access data sets for SFT(supervised fine tuning) 
There are theree categories of data sets:

1. **Dialog:** Each entry contains continous conversation
2.  **Pairs:** Each entry is an input-output pair
3.  **Context:** Each Entry has a context text and related QA pairs.


| Data set name | Released data | Data type | Size(Train+Test) | Description|
| :--- | :------: | ----: | ---: | ---: |
| [no_robots](https://huggingface.co/datasets/HuggingFaceH4/no_robots)        |   11/23   | SFT |9.5k + 0.5k|High quality human created SFT data set.|
|     [function_calling_extended](https://huggingface.co/datasets/Trelis/function_calling_extended) |08/23 | Pairs||High quality human created dataset from enhance LM's API using ability.|
| [Platypus](https://huggingface.co/datasets/garage-bAInd/Open-Platypus)    |  08/23   | Pairs |24.9k+0.1k|A very high quality dataset for improving LM's STEM reasoning ability.|
| [WebGLM-qa](https://huggingface.co/datasets/THUDM/webglm-qa/viewer/default/train) | 07/23  | Pairs |43.6k+1k|Dataset used by WebGLM, which is a QA system based on LLM and Internet. Each of the entry in this dataset comprise a question, a response and a reference. The response is grounded in the reference.|
|[dolphin](https://huggingface.co/datasets/ehartford/dolphin)|07/23|Pairs|4.5M|An attempt to replicate Microsoft's Orca. Based on FLANv2.|
|[openchat_share_gpt4](https://huggingface.co/datasets/openchat/openchat_sharegpt4_dataset)|07/23|Dialog|6k|A high quality dataset generated by using GPT-4 to complete refined ShareGPT prompts.|

For more open source data set, please visit this [website](https://github.com/Zjh-819/LLMDataHub?tab=readme-ov-file).

To know more about the existing open-source and close-source data sets and models, please visit this [paper](https://arxiv.org/pdf/2311.16989.pdf). 